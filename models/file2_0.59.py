import xgboost as xgb
import lightgbm as lgb
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, confusion_matrix
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gc

# 1. CHUáº¨N Bá»Š Dá»® LIá»†U
all_features = [c for c in X_full.columns if c not in ['object_id', 'target', 'split', 'SpecType', 'English Translation'] 
            and pd.api.types.is_numeric_dtype(X_full[c])]

X = X_full[all_features]
y = X_full['target']
X_test_raw = X_test_final[all_features]

print(f"ğŸ“¦ Dá»¯ liá»‡u gá»‘c: {X.shape[1]} Ä‘áº·c trÆ°ng")

# --- 2. GIAI ÄOáº N 1: CHá»ŒN Lá»ŒC Äáº¶C TRÆ¯NG (Lá»c rÃ¡c) ---
print("ğŸ” Äang cháº¡y Chá»n Lá»c Äáº·c TrÆ°ng (dÃ¹ng LightGBM nhanh)...")

# DÃ¹ng LightGBM Ä‘á»ƒ tÃ­nh importance nhanh
selector_model = lgb.LGBMClassifier(
    n_estimators=500, 
    learning_rate=0.05, 
    is_unbalance=True,
    verbose=-1,
    random_state=42
)

selector_model.fit(X, y)

# Chá»n features cÃ³ importance > ngÆ°á»¡ng trung bÃ¬nh (hoáº·c láº¥y Top K)
# á» Ä‘Ã¢y ta láº¥y Top 250 features tá»‘t nháº¥t Ä‘á»ƒ trÃ¡nh overfit
importances = selector_model.feature_importances_
indices = np.argsort(importances)[::-1] # Sáº¯p xáº¿p giáº£m dáº§n
top_k = 250 
top_features = [all_features[i] for i in indices[:top_k]]

print(f"âœ… ÄÃ£ chá»n {len(top_features)} features quan trá»ng nháº¥t.")
print(f"   Top 5: {top_features[:5]}")

# Cáº­p nháº­t láº¡i dá»¯ liá»‡u theo feature Ä‘Ã£ chá»n
X = X[top_features]
X_test = X_test_raw[top_features]

# --- 3. GIAI ÄOáº N 2: XGBOOST TRAINING (Táº­p trung GAIN) ---

# TÃ­nh tá»· lá»‡ Imbalance
scale_weight = np.sum(y == 0) / np.sum(y == 1)

xgb_params = {
    'n_estimators': 5000,           # Train sÃ¢u
    'learning_rate': 0.005,         # Há»c cháº­m
    'max_depth': 8,                 # CÃ¢y sÃ¢u hÆ¡n vÃ¬ Ä‘Ã£ lá»c feature rÃ¡c
    'subsample': 0.8,
    'colsample_bytree': 0.6,
    'objective': 'binary:logistic',
    'scale_pos_weight': scale_weight,
    'tree_method': 'hist',
    'n_jobs': -1,
    'random_state': 42,
    'reg_alpha': 1.0,               # L1 Regularization
    'reg_lambda': 3.0,              # L2 Regularization (cao Ä‘á»ƒ trÃ¡nh overfit)
    'eval_metric': 'aucpr',
    'importance_type': 'gain'
}

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
oof_preds = np.zeros(len(X))
test_preds = np.zeros(len(X_test))

print("\nğŸš€ Báº¯t Ä‘áº§u train XGBoost trÃªn táº­p features Ä‘Ã£ chá»n...")

for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):
    X_train, X_val = X.iloc[tr_idx], X.iloc[va_idx]
    y_train, y_val = y.iloc[tr_idx], y.iloc[va_idx]

    model = xgb.XGBClassifier(**xgb_params)
    
    # Early Stopping thá»§ cÃ´ng (do version má»›i sklearn Ä‘á»•i API)
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False,
    )
    
    # Predict
    val_pred = model.predict_proba(X_val)[:, 1]
    oof_preds[va_idx] = val_pred
    test_preds += model.predict_proba(X_test)[:, 1] / kf.n_splits
    
    # Check F1
    score = f1_score(y_val, (val_pred > 0.ğŸ˜.astype(int)) # Check táº¡m threshold cao
    print(f"   Fold {fold+1}: F1 ~ {score:.4f} | Best Iter: {model.best_iteration if hasattr(model, 'best_iteration') else 'N/A'}")
    
    del model, X_train, X_val
    gc.collect()

# --- 4. Tá»I Æ¯U NGÆ¯á» NG & FILE Ná»˜P BÃ€I ---
print("\nğŸšï¸ Äang dÃ² tÃ¬m ngÆ°á»¡ng tá»‘i Æ°u (Threshold Tuning)...")
best_f1 = 0
best_t = 0.5
for t in np.arange(0.1, 0.99, 0.005):
    score = f1_score(y, (oof_preds > t).astype(int))
    if score > best_f1:
        best_f1 = score
        best_t = t

print("="*40)
print(f"ğŸ† FINAL F1: {best_f1:.4f} @ Threshold {best_t:.3f}")
print("="*40)

# Xuáº¥t file
sub = pd.DataFrame({
    'object_id': X_test_final['object_id'],
    'target': (test_preds > best_t).astype(int)
})
sub.to_csv("submission_massive_select.csv", index=False)
print(f"âœ… ÄÃ£ lÆ°u file: submission_massive_select.csv (TDEs: {sub['target'].sum()})")

# Váº½ Äá»™ Quan Trá»ng Äáº·c TrÆ°ng
plt.figure(figsize=(10, 15))
# Láº¥y importance tá»« láº§n cháº¡y cuá»‘i (hoáº·c cÃ³ thá»ƒ tÃ­ch lÅ©y)
# LÆ°u Ã½: ÄÃ¢y lÃ  importance sau khi Ä‘Ã£ lá»c
sns.barplot(x=model.feature_importances_[:30], y=top_features[:30])
plt.title("Top 30 Selected Features (XGBoost Gain)")
plt.show()